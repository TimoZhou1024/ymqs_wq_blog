---
title: Weclone配置教程
date: \today
tags: 
  - Wechat
  - Weclone
  - github
categories: 
  - tech
---

# WeClone配置教程

## 1. 环境准备

### 在Linux(WSL)环境下安装UV
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 在Windows环境下安装UV
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

## 2. 详细配置步骤

### 2.1 环境准备
- 确保在AutoDL等云环境下配置网络加速（如果有）
```bash
source /etc/network_turbo
```

### 2.2 安装UV包管理器
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env 
```

### 2.3 克隆WeClone仓库
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
```

### 2.4 创建并激活虚拟环境
```bash
uv venv .venv --python=3.10
source .venv/bin/activate
```

### 2.5 安装依赖
```bash
uv pip install --group main -e .
uv pip install -e .
```

### 2.6 验证CUDA环境
```bash
python -c "import torch; print('CUDA是否可用:', torch.cuda.is_available()); print('CUDA版本:', torch.version.cuda); print('PyTorch版本:', torch.__version__)"
python --version
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
nvcc -V
```

### 2.7 配置WeClone设置
```bash
cp settings.template.jsonc settings.jsonc
# 根据需要编辑settings.jsonc文件，配置微信相关设置
```

### 2.8 安装Git LFS（用于大模型下载）
```bash
sudo apt-get update
sudo apt-get install git-lfs
git lfs install
```

### 2.9 安装LLaMA-Factory（用于模型训练）
```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
uv pip install -e ".[torch,metrics]"
cd ..
llamafactory-cli version
```

### 2.10 下载基础模型
```bash
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```

### 2.11 WeClone主要操作流程
```bash
# 生成训练数据集
weclone-cli make-dataset

# 训练SFT模型
weclone-cli train-sft

# 启动Web聊天演示
weclone-cli webchat-demo
```

## 3. 本地访问

AutoDL没有固定的公网ip，因此通过ip+端口（一般是7860）访问是不可行的，参考官方的说明[port](https://www.autodl.com/docs/port/)，在服务界面选择`自定义服务`：

![自定义服务](https://lsky.ymqs.top/i/2025/05/17/682776a56897e.png)

安装相关的工具（[AutoDL-SSH-Tools.zip](https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/AutoDL-SSH-Tools.zip)）：

![Tools](https://lsky.ymqs.top/i/2025/05/17/682776d87dbc0.png)

下载完成后解压，打开文件夹下的可执行文件`AutoDL.exe`，输入登录指令，密码和你要代理的端口（这里应该是7860）:

![AutoDL.exe](https://lsky.ymqs.top/i/2025/05/17/68277752797e0.png)

然后在本地浏览器里打开[WebUI](http://127.0.0.1:7860/)，即可启动Web聊天演示。

## 4. 可选步骤

### 安装flash-attention（提高训练速度，实测有问题故放弃使用）
```bash
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
uv pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
```

## 5. 常见问题解决

### 5.1 CUDA不可用的问题
- 检查NVIDIA驱动是否正确安装
- 确认PyTorch版本与CUDA版本匹配

### 5.2 内存不足问题
- 减小batch_size
- 使用gradient_checkpointing
- 考虑使用更小的模型

### 5.3 数据集生成问题
- 确保微信聊天记录可正确获取
- 检查settings.jsonc配置是否正确

## 6. WSL清理

```bash
diskpart
select vdisk file = "D:\wsl\Ubuntu\ext4.vhdx"
compact vdisk
exit
```

## 7. 完整安装脚本

```bash
# 用于AutoDL环境的完整安装脚本
source /etc/network_turbo

# 安装UV包管理器
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env 

# 克隆WeClone仓库并设置环境
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate
uv pip install --group main -e .
uv pip install -e .

# 验证环境
python -c "import torch; print('CUDA是否可用:', torch.cuda.is_available()); print('CUDA版本:', torch.version.cuda); print('PyTorch版本:', torch.__version__)"
cp settings.template.jsonc settings.jsonc
python --version
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
nvcc -V

# 配置Git LFS
sudo apt-get update
sudo apt-get install git-lfs
git lfs install

# 安装LLaMA-Factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
uv pip install -e ".[torch,metrics]"
cd ..
llamafactory-cli version

# 下载基础模型
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git

# WeClone主要操作流程
weclone-cli make-dataset  # 生成训练数据集
weclone-cli train-sft     # 训练SFT模型
weclone-cli webchat-demo  # 启动Web聊天演示
```

## 8. 参考链接

- [Weclone GitHub 仓库](https://github.com/xming521/WeClone)
- [PyWxDump GitHub 仓库](https://github.com/xaoyaoo/PyWxDump)
- [AutoDL 端口映射文档](https://www.autodl.com/docs/port/)