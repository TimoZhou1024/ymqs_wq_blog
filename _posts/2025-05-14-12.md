---
title: Weclone配置教程
date: \today
tags: 
- Wechat
- Weclone
- github
categories: 
- tech
---


## WeClone配置步骤

1. **环境准备**
   - 确保在AutoDL等云环境下配置网络加速（如果有）
   ```bash
   source /etc/network_turbo
   ```

2. **安装uv包管理器**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   source $HOME/.local/bin/env 
   ```

3. **克隆WeClone仓库**
   ```bash
   git clone https://github.com/xming521/WeClone.git
   cd WeClone
   ```

4. **创建并激活虚拟环境**
   ```bash
   uv venv .venv --python=3.10
   source .venv/bin/activate
   ```

5. **安装依赖**
   ```bash
   uv pip install --group main -e .
   uv pip install -e .
   ```

6. **验证CUDA环境**
   ```bash
   python -c "import torch; print('CUDA是否可用:', torch.cuda.is_available()); print('CUDA版本:', torch.version.cuda); print('PyTorch版本:', torch.__version__)"
   python --version
   python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
   nvcc -V
   ```

7. **配置WeClone设置**
   ```bash
   cp settings.template.jsonc settings.jsonc
   # 根据需要编辑settings.jsonc文件，配置微信相关设置
   ```

8. **安装Git LFS（用于大模型下载）**
   ```bash
   sudo apt-get update
   sudo apt-get install git-lfs
   git lfs install
   ```

9. **安装LLaMA-Factory（用于模型训练）**
   ```bash
   git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
   cd LLaMA-Factory
   uv pip install -e ".[torch,metrics]"
   cd ..
   llamafactory-cli version
   ```

10. **下载基础模型**
    ```bash
    git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
    ```

11. **WeClone主要操作流程**
    ```bash
    # 生成训练数据集
    weclone-cli make-dataset
    
    # 训练SFT模型
    weclone-cli train-sft
    
    # 启动Web聊天演示
    weclone-cli webchat-demo
    ```

### 可选步骤

1. **安装flash-attention（可选，提高训练速度，实测有问题故放弃使用）**
   ```bash
   wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
   uv pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
   ```

## 常见问题解决

1. **CUDA不可用的问题**
   - 检查NVIDIA驱动是否正确安装
   - 确认PyTorch版本与CUDA版本匹配

2. **内存不足问题**
   - 减小batch_size
   - 使用gradient_checkpointing
   - 考虑使用更小的模型

3. **数据集生成问题**
   - 确保微信聊天记录可正确获取
   - 检查settings.jsonc配置是否正确

## wsl清理

```bash
diskpart
select vdisk file = "D:\wsl\Ubuntu\ext4.vhdx"
compact vdisk
exit
```

## 精简脚本
```bash
#used for autodl
source /etc/network_turbo

curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env 
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate
uv pip install --group main -e .
uv pip install -e .
python -c "import torch; print('CUDA是否可用:', torch.cuda.is_available()); print('CUDA版本:', torch.version.cuda); print('PyTorch版本:', torch.__version__)"
cp settings.template.jsonc settings.jsonc

python --version &&
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())" &&
nvcc -V


# wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
# uv pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

sudo apt-get update
sudo apt-get install git-lfs
git lfs install

git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
uv pip install -e ".[torch,metrics]"
cd ..
# pip install --no-deps -e .
llamafactory-cli version

git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git

# 生成训练数据集
weclone-cli make-dataset

# 训练SFT模型
weclone-cli train-sft

# 启动Web聊天演示
weclone-cli webchat-demo
```

## 参考链接

- [Weclone](https://github.com/xming521/WeClone)
- [PyWxDump](https://github.com/xaoyaoo/PyWxDump)